5k-Results-fasttext-cbow-cosine-cca-java
Only binary threshold, no nearest neighbors.  No reverse.
values only for 25 and 50.

5k-Results-fasttext-skipgram-cosine-cca-java
Only binary threshold, no nearest neighbors.

Reverse only for 50, 75, 250
Forward only for 25, 50, 75, 100, 150; still trending upward at 150, for the ranking task, not so much for binary.


Results-fasttext-cbow-arcos-cca-java
Since Binary Threshold and Nearest Neighbor strategies apply only to binary 
task, not to ranking task, it isn't very surprising that the Bi-Fo and Ne-Fo
runs have similar results for the ranking task; likewise for Bi-Re and Ne-Re.

For the binary task, nearest neighbors is better for fasttext-cbow-arcos
Results-fasttext-cbow-cosine-cca-java
Results-fasttext-skipgram-arcos-cca-java
Results-fasttext-skipgram-cosine-cca-java
Results-w2v-cbow-arcos-cca-java
Results-w2v-cbow-cosine-cca-java
Results-w2v-skipgram-arcos-cca-java
Results-w2v-skipgram-cosine-cca-java

<par>SUMMARY. The skipgram statistics seem to generally decline for embedding 
dimension greater than 150, whereas the effect isn't so marked for CBOW.
(CBOW scores are a little lower.)
But the errorbars are pretty wide, so it's hard to trust tendencies.   
(I used your confidence interval for the errorbars.  
Increasing the number of runs would narrow the confidence interval, I think.)

<par>
One way to increase the number of runs for the same effort would be to use
a single embedding for several different runs, for example
one embedding could be used for: both Reverse and Forward runs, both Nearest Neighbor and Binary threshold, cosine and arcos -- that's eight runs for one 
embedding; and building the embedding is much more expensive than the
comparisons.

<par>
You collected the statistics, but I didn't graph individual languages 
for Reverse embeddings.   It seems likely that there is a relationship
between corpus sizes and when Reverse embeddings give better scores, but 
maybe our small number of corpus sizes won't let us tease it out.  However,
we could artificially limit the size of the German corpus, which is pretty
big, and see if we could learn anything.
